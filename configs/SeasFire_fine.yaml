seed_everything: 42

# ---------------------------- TRAINER -------------------------------------------
trainer:
  default_root_dir: ${oc.env:OUTPUT_DIR,/home/as26840@ens.ad.etsmtl.ca/repos/Wildfire_Bench/logs/SeasFire_fine}

  precision: 16

  gpus: null
  num_nodes: 1
  accelerator: gpu
  strategy: ddp

  min_epochs: 40
  max_epochs: 100
  enable_progress_bar: true

  sync_batchnorm: True
  enable_checkpointing: True
  resume_from_checkpoint: null

  # debugging
  fast_dev_run: false

  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      project: SeasFire_fine_WFB
      name: test
      log_model: true

  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: "val/loss" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        patience: 100 # how many validation epochs of not improving until training stops
        min_delta: 0. # minimum change in the monitored metric needed to qualify as an improvement

    - class_path: pytorch_lightning.callbacks.RichModelSummary
      init_args:
        max_depth: -1

    - class_path: pytorch_lightning.callbacks.RichProgressBar

# ---------------------------- MODEL -------------------------------------------
model:
  lr: 5e-7
  beta_1: 0.9
  beta_2: 0.99
  weight_decay: 1e-5
  warmup_epochs: 10000
  max_epochs: 100000
  warmup_start_lr: 1e-8
  eta_min: 1e-8
  pos_class_weight: 236 # for focal loss, need to figure out for other data (relative frequency in augmented 2018 and 2020 fires)^-1
  loss_function: "BCE"
  pretrained_res: "1.40625deg"

  net:
    class_path: models.climax.arch.ClimaX
    init_args:
      default_vars: [
          'lst_day',
          'mslp',
          'ndvi',
          'pop_dens',
          'ssrd',
          'sst',
          'swvl1',
          't2m_mean',
          'tp',
          'vpd',
          'gwis_ba'
            ]
      img_size: [128, 128]
      output_size: [128, 128]
      patch_size: 2
      embed_dim: 1024
      depth: 8
      decoder_depth: 2
      num_heads: 16
      mlp_ratio: 4
      drop_path: 0.1
      drop_rate: 0.1

# ---------------------------- DATA -------------------------------------------
data:
  root_dir: /home/as26840@ens.ad.etsmtl.ca/data/seasfire/SeasFireCube_v3.zarr
  variables: [
      'lst_day',
      'mslp',
      'ndvi',
      'pop_dens',
      'ssrd',
      'sst',
      'swvl1',
      't2m_mean',
      'tp',
      'vpd'
      ]
  out_variables: 'gwis_ba'
  predict_range: 192
  hrs_each_step: 1 # play around with this later, unsure if its supposed to be 24 since dataset resolution is 24
  batch_size: 2
  num_workers: 1
  pin_memory: False
  positional_vars: ['cos_lat', 'sin_lat', 'cos_lon', 'sin_lon']

  crop_side_length: 128